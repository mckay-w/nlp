{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 数据同步"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Retry=9, Wait=0.1, Timestamp=1710773192.9471455\n",
      "WARNING:root:Retry=8, Wait=0.2, Timestamp=1710773193.0666716\n",
      "WARNING:root:Retry=7, Wait=0.4, Timestamp=1710773193.2880187\n",
      "WARNING:root:Retry=6, Wait=0.8, Timestamp=1710773193.707694\n",
      "WARNING:root:Retry=5, Wait=1.6, Timestamp=1710773194.5300114\n",
      "WARNING:root:Retry=4, Wait=3.2, Timestamp=1710773196.154472\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-a13055c1b9e8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmoxing\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mmox\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# 请替换成自己的obs路径\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmox\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy_parallel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc_url\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"s3://ascend-zyjs-dcyang/nlp/text_classification_mindspore/data/\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdst_url\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'./data/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/MindSpore/lib/python3.7/site-packages/moxing/framework/file/file_io.py\u001b[0m in \u001b[0;36mcopy_parallel\u001b[0;34m(src_url, dst_url, file_list, threads, is_processing, use_queue, atom, skip_not_found, create_obs_dir)\u001b[0m\n\u001b[1;32m   2403\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2404\u001b[0m   \u001b[0mcopy_parallel_start\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2405\u001b[0;31m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_directory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc_url\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2406\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc_url\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdst_url\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0matom\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0matom\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2407\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/MindSpore/lib/python3.7/site-packages/moxing/framework/util/runtime.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 259\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    260\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mfunctools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwraps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/MindSpore/lib/python3.7/site-packages/moxing/framework/file/file_io.py\u001b[0m in \u001b[0;36mis_directory\u001b[0;34m(url)\u001b[0m\n\u001b[1;32m   1680\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1681\u001b[0m   \u001b[0mobs_client\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_create_or_get_obs_client\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1682\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0m_is_directory_obs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs_client\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbucket_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobject_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1683\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1684\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/MindSpore/lib/python3.7/site-packages/moxing/framework/file/file_io.py\u001b[0m in \u001b[0;36m_is_directory_obs\u001b[0;34m(obs_client, bucket_name, object_key)\u001b[0m\n\u001b[1;32m   1686\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mobject_key\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_PROTOCOL_SEP\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1687\u001b[0m     \u001b[0mobject_key\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobject_key\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0m_PROTOCOL_SEP\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1688\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0m_exists_obs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs_client\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbucket_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobject_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1689\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1690\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/MindSpore/lib/python3.7/site-packages/moxing/framework/file/file_io.py\u001b[0m in \u001b[0;36m_exists_obs\u001b[0;34m(obs_client, bucket_name, object_key)\u001b[0m\n\u001b[1;32m   1637\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1638\u001b[0m     resp_file = _retryable_call(obs_client, 'getObjectMetadata', bucket_name,\n\u001b[0;32m-> 1639\u001b[0;31m                                 object_key, ignore_not_found=True)\n\u001b[0m\u001b[1;32m   1640\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mresp_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m300\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1641\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0m_USE_METADATA_CACHE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/MindSpore/lib/python3.7/site-packages/moxing/framework/file/file_io.py\u001b[0m in \u001b[0;36m_retryable_call\u001b[0;34m(obs_client, func_name, *args, **kwargs)\u001b[0m\n\u001b[1;32m    712\u001b[0m       \u001b[0mretry\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    713\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mretry_wait\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 714\u001b[0;31m         \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretry_wait\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    715\u001b[0m         \u001b[0mretry_wait\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretry_wait\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_RETRY_MAX_WAIT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    716\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import moxing as mox\n",
    "# 请替换成自己的obs路径\n",
    "mox.file.copy_parallel(src_url=\"s3://ascend-zyjs-dcyang/nlp/text_classification_mindspore/data/\", dst_url='./data/') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 导入依赖库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import math\n",
    "import random\n",
    "import codecs\n",
    "from pathlib import Path\n",
    "\n",
    "import mindspore\n",
    "import mindspore.dataset as ds\n",
    "import mindspore.nn as nn\n",
    "from mindspore import Tensor\n",
    "from mindspore import context\n",
    "from mindspore.train.model import Model\n",
    "from mindspore.nn.metrics import Accuracy\n",
    "from mindspore.train.serialization import load_checkpoint, load_param_into_net\n",
    "from mindspore.train.callback import ModelCheckpoint, CheckpointConfig, LossMonitor, TimeMonitor\n",
    "from mindspore.ops import operations as ops"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 超参数设置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from easydict import EasyDict as edict\n",
    "\n",
    "cfg = edict({\n",
    "    'name': 'movie review',\n",
    "    'pre_trained': False,\n",
    "    'num_classes': 2,\n",
    "    'batch_size': 64,\n",
    "    'epoch_size': 8,\n",
    "    'weight_decay': 3e-5,\n",
    "    'data_path': './data/',\n",
    "    'device_target': 'Ascend',\n",
    "    'device_id': 0,\n",
    "    'keep_checkpoint_max': 1,\n",
    "    'checkpoint_path': './ckpt/train_textcnn-4_149.ckpt',\n",
    "    'word_len': 51,\n",
    "    'vec_length': 40\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "context.set_context(mode=context.GRAPH_MODE, device_target=cfg.device_target, device_id=cfg.device_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 数据预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negative reivews:\n",
      "[0]:simplistic , silly and tedious . \n",
      "\n",
      "[1]:it's so laddish and juvenile , only teenage boys could possibly find it funny . \n",
      "\n",
      "[2]:exploitative and largely devoid of the depth or sophistication that would make watching such a graphic treatment of the crimes bearable . \n",
      "\n",
      "[3]:[garbus] discards the potential for pathological study , exhuming instead , the skewed melodrama of the circumstantial situation . \n",
      "\n",
      "[4]:a visually flashy but narratively opaque and emotionally vapid exercise in style and mystification . \n",
      "\n",
      "Positive reivews:\n",
      "[0]:the rock is destined to be the 21st century's new \" conan \" and that he's going to make a splash even greater than arnold schwarzenegger , jean-claud van damme or steven segal . \n",
      "\n",
      "[1]:the gorgeously elaborate continuation of \" the lord of the rings \" trilogy is so huge that a column of words cannot adequately describe co-writer/director peter jackson's expanded vision of j . r . r . tolkien's middle-earth . \n",
      "\n",
      "[2]:effective but too-tepid biopic\n",
      "\n",
      "[3]:if you sometimes like to go to the movies to have fun , wasabi is a good place to start . \n",
      "\n",
      "[4]:emerges as something rare , an issue movie that's so honest and keenly observed that it doesn't feel like one . \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 数据预览\n",
    "with open(\"./data/rt-polarity.neg\", 'r', encoding='utf-8') as f:\n",
    "        print(\"Negative reivews:\")\n",
    "        for i in range(5):\n",
    "            print(\"[{0}]:{1}\".format(i,f.readline()))\n",
    "with open(\"./data/rt-polarity.pos\", 'r', encoding='utf-8') as f:\n",
    "        print(\"Positive reivews:\")\n",
    "        for i in range(5):\n",
    "            print(\"[{0}]:{1}\".format(i,f.readline()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator():\n",
    "    def __init__(self, input_list):\n",
    "        self.input_list=input_list\n",
    "    def __getitem__(self,item):\n",
    "        return (np.array(self.input_list[item][0],dtype=np.int32),\n",
    "                np.array(self.input_list[item][1],dtype=np.int32))\n",
    "    def __len__(self):\n",
    "        return len(self.input_list)\n",
    "\n",
    "\n",
    "class MovieReview:\n",
    "    '''\n",
    "    影评数据集\n",
    "    '''\n",
    "    def __init__(self, root_dir, maxlen, split):\n",
    "        '''\n",
    "        input:\n",
    "            root_dir: 影评数据目录\n",
    "            maxlen: 设置句子最大长度\n",
    "            split: 设置数据集中训练/评估的比例\n",
    "        '''\n",
    "        self.path = root_dir\n",
    "        self.feelMap = {\n",
    "            'neg':0,\n",
    "            'pos':1\n",
    "        }\n",
    "        self.files = []\n",
    "\n",
    "        self.doConvert = False\n",
    "        \n",
    "        mypath = Path(self.path)\n",
    "        if not mypath.exists() or not mypath.is_dir():\n",
    "            print(\"please check the root_dir!\")\n",
    "            raise ValueError\n",
    "\n",
    "        # 在数据目录中找到文件\n",
    "        for root,_,filename in os.walk(self.path):\n",
    "            for each in filename:\n",
    "                self.files.append(os.path.join(root,each))\n",
    "            break\n",
    "\n",
    "        # 确认是否为两个文件.neg与.pos\n",
    "        if len(self.files) != 2:\n",
    "            print(\"There are {} files in the root_dir\".format(len(self.files)))\n",
    "            raise ValueError\n",
    "\n",
    "        # 读取数据\n",
    "        self.word_num = 0\n",
    "        self.maxlen = 0\n",
    "        self.minlen = float(\"inf\")\n",
    "        self.maxlen = float(\"-inf\")\n",
    "        self.Pos = []\n",
    "        self.Neg = []\n",
    "        for filename in self.files:\n",
    "            f = codecs.open(filename, 'r')\n",
    "            ff = f.read()\n",
    "            file_object = codecs.open(filename, 'w', 'utf-8')\n",
    "            file_object.write(ff)\n",
    "            self.read_data(filename)\n",
    "        self.PosNeg = self.Pos + self.Neg\n",
    "\n",
    "        self.text2vec(maxlen=maxlen)\n",
    "        self.split_dataset(split=split)\n",
    "\n",
    "    def read_data(self, filePath):\n",
    "\n",
    "        with open(filePath,'r') as f:\n",
    "            \n",
    "            for sentence in f.readlines():\n",
    "                sentence = sentence.replace('\\n','')\\\n",
    "                                    .replace('\"','')\\\n",
    "                                    .replace('\\'','')\\\n",
    "                                    .replace('.','')\\\n",
    "                                    .replace(',','')\\\n",
    "                                    .replace('[','')\\\n",
    "                                    .replace(']','')\\\n",
    "                                    .replace('(','')\\\n",
    "                                    .replace(')','')\\\n",
    "                                    .replace(':','')\\\n",
    "                                    .replace('--','')\\\n",
    "                                    .replace('-',' ')\\\n",
    "                                    .replace('\\\\','')\\\n",
    "                                    .replace('0','')\\\n",
    "                                    .replace('1','')\\\n",
    "                                    .replace('2','')\\\n",
    "                                    .replace('3','')\\\n",
    "                                    .replace('4','')\\\n",
    "                                    .replace('5','')\\\n",
    "                                    .replace('6','')\\\n",
    "                                    .replace('7','')\\\n",
    "                                    .replace('8','')\\\n",
    "                                    .replace('9','')\\\n",
    "                                    .replace('`','')\\\n",
    "                                    .replace('=','')\\\n",
    "                                    .replace('$','')\\\n",
    "                                    .replace('/','')\\\n",
    "                                    .replace('*','')\\\n",
    "                                    .replace(';','')\\\n",
    "                                    .replace('<b>','')\\\n",
    "                                    .replace('%','')\n",
    "                sentence = sentence.split(' ')\n",
    "                sentence = list(filter(lambda x: x, sentence))\n",
    "                if sentence:\n",
    "                    self.word_num += len(sentence)\n",
    "                    self.maxlen = self.maxlen if self.maxlen >= len(sentence) else len(sentence)\n",
    "                    self.minlen = self.minlen if self.minlen <= len(sentence) else len(sentence)\n",
    "                    if 'pos' in filePath:\n",
    "                        self.Pos.append([sentence,self.feelMap['pos']])\n",
    "                    else:\n",
    "                        self.Neg.append([sentence,self.feelMap['neg']])\n",
    "\n",
    "    def text2vec(self, maxlen):\n",
    "        '''\n",
    "        将句子转化为向量\n",
    "\n",
    "        '''\n",
    "        # Vocab = {word : index}\n",
    "        self.Vocab = dict()\n",
    "\n",
    "        # self.Vocab['None']\n",
    "        for SentenceLabel in self.Pos+self.Neg:\n",
    "            vector = [0]*maxlen\n",
    "            for index, word in enumerate(SentenceLabel[0]):\n",
    "                if index >= maxlen:\n",
    "                    break\n",
    "                if word not in self.Vocab.keys():\n",
    "                    self.Vocab[word] = len(self.Vocab)\n",
    "                    vector[index] = len(self.Vocab) - 1\n",
    "                else:\n",
    "                    vector[index] = self.Vocab[word]\n",
    "            SentenceLabel[0] = vector\n",
    "        self.doConvert = True\n",
    "\n",
    "    def split_dataset(self, split):\n",
    "        '''\n",
    "        分割为训练集与测试集\n",
    "\n",
    "        '''\n",
    "\n",
    "        trunk_pos_size = math.ceil((1-split)*len(self.Pos))\n",
    "        trunk_neg_size = math.ceil((1-split)*len(self.Neg))\n",
    "        trunk_num = int(1/(1-split))\n",
    "        pos_temp=list()\n",
    "        neg_temp=list()\n",
    "        for index in range(trunk_num):\n",
    "            pos_temp.append(self.Pos[index*trunk_pos_size:(index+1)*trunk_pos_size])\n",
    "            neg_temp.append(self.Neg[index*trunk_neg_size:(index+1)*trunk_neg_size])\n",
    "        self.test = pos_temp.pop(2)+neg_temp.pop(2)\n",
    "        self.train = [i for item in pos_temp+neg_temp for i in item]\n",
    "\n",
    "        random.shuffle(self.train)\n",
    "        # random.shuffle(self.test)\n",
    "\n",
    "    def get_dict_len(self):\n",
    "        '''\n",
    "        获得数据集中文字组成的词典长度\n",
    "        '''\n",
    "        if self.doConvert:\n",
    "            return len(self.Vocab)\n",
    "        else:\n",
    "            print(\"Haven't finished Text2Vec\")\n",
    "            return -1\n",
    "\n",
    "    def create_train_dataset(self, epoch_size, batch_size):\n",
    "        dataset = ds.GeneratorDataset(\n",
    "                                        source=Generator(input_list=self.train), \n",
    "                                        column_names=[\"data\",\"label\"], \n",
    "                                        shuffle=False\n",
    "                                        )\n",
    "#         dataset.set_dataset_size(len(self.train))\n",
    "        dataset=dataset.batch(batch_size=batch_size,drop_remainder=True)\n",
    "        dataset=dataset.repeat(epoch_size)\n",
    "        return dataset\n",
    "\n",
    "    def create_test_dataset(self, batch_size):\n",
    "        dataset = ds.GeneratorDataset(\n",
    "                                        source=Generator(input_list=self.test), \n",
    "                                        column_names=[\"data\",\"label\"], \n",
    "                                        shuffle=False\n",
    "                                        )\n",
    "#         dataset.set_dataset_size(len(self.test))\n",
    "        dataset=dataset.batch(batch_size=batch_size,drop_remainder=True)\n",
    "        return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "instance = MovieReview(root_dir=cfg.data_path, maxlen=cfg.word_len, split=0.9)\n",
    "dataset = instance.create_train_dataset(batch_size=cfg.batch_size,epoch_size=cfg.epoch_size)\n",
    "batch_num = dataset.get_dataset_size() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab_size:18848\n",
      "{'data': Tensor(shape=[64, 51], dtype=Int32, value=\n",
      "[[  56, 7234,    4 ...    0,    0,    0],\n",
      " [ 302,  339,   65 ...    0,    0,    0],\n",
      " [  59,   15,  912 ...    0,    0,    0],\n",
      " ...\n",
      " [ 145, 1517,   15 ...    0,    0,    0],\n",
      " [   0,    8, 1918 ...    0,    0,    0],\n",
      " [1770,   94,    0 ...    0,    0,    0]]), 'label': Tensor(shape=[64], dtype=Int32, value= [0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, \n",
      " 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, \n",
      " 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0])}\n",
      "[ 302  339   65  246  636  145 2138  243   15   77  122   15  491  946\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0]\n"
     ]
    }
   ],
   "source": [
    "vocab_size=instance.get_dict_len()\n",
    "print(\"vocab_size:{0}\".format(vocab_size))\n",
    "item =dataset.create_dict_iterator()\n",
    "for i,data in enumerate(item):\n",
    "    if i<1:\n",
    "        print(data)\n",
    "        print(data['data'][1])\n",
    "    else:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.模型训练"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1训练参数设置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = []\n",
    "warm_up = [1e-3 / math.floor(cfg.epoch_size / 5) * (i + 1) for _ in range(batch_num) \n",
    "           for i in range(math.floor(cfg.epoch_size / 5))]\n",
    "shrink = [1e-3 / (16 * (i + 1)) for _ in range(batch_num) \n",
    "          for i in range(math.floor(cfg.epoch_size * 3 / 5))]\n",
    "normal_run = [1e-3 for _ in range(batch_num) for i in \n",
    "              range(cfg.epoch_size - math.floor(cfg.epoch_size / 5) \n",
    "                    - math.floor(cfg.epoch_size * 2 / 5))]\n",
    "learning_rate = learning_rate + warm_up + normal_run + shrink"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _weight_variable(shape, factor=0.01):\n",
    "    init_value = np.random.randn(*shape).astype(np.float32) * factor\n",
    "    return Tensor(init_value)\n",
    "\n",
    "\n",
    "def make_conv_layer(kernel_size):\n",
    "    weight_shape = (96, 1, *kernel_size)\n",
    "    weight = _weight_variable(weight_shape)\n",
    "    return nn.Conv2d(in_channels=1, out_channels=96, kernel_size=kernel_size, padding=1,\n",
    "                     pad_mode=\"pad\", weight_init=weight, has_bias=True)\n",
    "\n",
    "\n",
    "class TextCNN(nn.Cell):\n",
    "    def __init__(self, vocab_len, word_len, num_classes, vec_length):\n",
    "        super(TextCNN, self).__init__()\n",
    "        self.vec_length = vec_length\n",
    "        self.word_len = word_len\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        self.unsqueeze = ops.ExpandDims()\n",
    "        self.embedding = nn.Embedding(vocab_len, self.vec_length, embedding_table='normal')\n",
    "\n",
    "        self.slice = ops.Slice()\n",
    "        self.layer1 = self.make_layer(kernel_height=3)\n",
    "        self.layer2 = self.make_layer(kernel_height=4)\n",
    "        self.layer3 = self.make_layer(kernel_height=5)\n",
    "\n",
    "        self.concat = ops.Concat(1)\n",
    "\n",
    "        self.fc = nn.Dense(96*3, self.num_classes)\n",
    "        self.drop = nn.Dropout(keep_prob=0.5)\n",
    "        self.print = ops.Print()\n",
    "        self.reducemean = ops.ReduceMax(keep_dims=False)\n",
    "        \n",
    "    def make_layer(self, kernel_height):\n",
    "        return nn.SequentialCell(\n",
    "            [\n",
    "                make_conv_layer((kernel_height,self.vec_length)),\n",
    "                nn.ReLU(),\n",
    "                nn.MaxPool2d(kernel_size=(self.word_len-kernel_height+1,1)),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def construct(self,x):\n",
    "        x = self.unsqueeze(x, 1)\n",
    "        x = self.embedding(x)\n",
    "        x1 = self.layer1(x)\n",
    "        x2 = self.layer2(x)\n",
    "        x3 = self.layer3(x)\n",
    "\n",
    "        x1 = self.reducemean(x1, (2, 3))\n",
    "        x2 = self.reducemean(x2, (2, 3))\n",
    "        x3 = self.reducemean(x3, (2, 3))\n",
    "\n",
    "        x = self.concat((x1, x2, x3))\n",
    "        x = self.drop(x)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = TextCNN(vocab_len=instance.get_dict_len(), word_len=cfg.word_len, \n",
    "              num_classes=cfg.num_classes, vec_length=cfg.vec_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TextCNN<\n",
      "  (embedding): Embedding<vocab_size=18848, embedding_size=40, use_one_hot=False, embedding_table=Parameter (name=embedding.embedding_table, shape=(18848, 40), dtype=Float32, requires_grad=True), dtype=Float32, padding_idx=None>\n",
      "  (layer1): SequentialCell<\n",
      "    (0): Conv2d<input_channels=1, output_channels=96, kernel_size=(3, 40), stride=(1, 1), pad_mode=pad, padding=1, dilation=(1, 1), group=1, has_bias=True, weight_init=[[[[-1.40142590e-02 -1.39044225e-02  1.76400773e-03 ...  1.98193011e-03\n",
      "         3.29852314e-03  1.11813694e-02]\n",
      "       [-7.84928910e-04 -8.33186693e-03 -2.79705878e-03 ...  1.24771930e-02\n",
      "        -9.19989124e-03 -1.38720470e-02]\n",
      "       [-1.23752991e-03 -1.14431763e-02 -3.59331141e-03 ...  1.99491512e-02\n",
      "        -1.06524164e-02 -6.11903658e-03]]]\n",
      "    \n",
      "    \n",
      "     [[[-4.77746548e-03 -4.26662294e-03 -8.04466009e-03 ... -6.40575448e-03\n",
      "         4.20129905e-03 -1.72102661e-03]\n",
      "       [ 1.27870329e-02 -1.71797851e-03 -3.33426078e-03 ... -1.36601590e-02\n",
      "        -1.18772192e-02 -2.86587682e-02]\n",
      "       [-1.07312212e-02  6.38615852e-03  5.12786675e-03 ... -8.52044486e-03\n",
      "         1.85460446e-03  9.44645377e-04]]]\n",
      "    \n",
      "    \n",
      "     [[[-1.09158009e-02  9.28521995e-03 -1.62181985e-02 ... -2.17618677e-03\n",
      "         6.25687465e-03  1.48888957e-02]\n",
      "       [-7.10228411e-03 -8.17214139e-03 -1.27893523e-03 ... -3.84730916e-03\n",
      "        -1.94855630e-02  7.68538448e-04]\n",
      "       [-1.01149734e-02  2.09053252e-02  1.16464274e-03 ... -6.30160188e-03\n",
      "         1.77806299e-02 -1.71709098e-02]]]\n",
      "    \n",
      "    \n",
      "     ...\n",
      "    \n",
      "    \n",
      "     [[[-1.61314756e-02 -3.85430292e-03  5.76364342e-03 ... -8.62755254e-03\n",
      "         3.97900258e-05 -3.33739980e-03]\n",
      "       [-5.48644178e-03 -2.55374151e-04  1.01305284e-02 ... -6.38139155e-03\n",
      "        -1.85046494e-02 -2.07101312e-02]\n",
      "       [ 9.24957509e-04 -1.56174460e-03 -8.53967667e-03 ... -2.69678101e-04\n",
      "        -1.06310686e-02 -7.89619703e-03]]]\n",
      "    \n",
      "    \n",
      "     [[[-3.91785009e-03 -3.84797342e-04 -1.09296590e-02 ... -9.61130206e-03\n",
      "        -2.59427889e-03  2.44608312e-03]\n",
      "       [-1.22069602e-03  8.39091558e-03 -9.00769047e-03 ...  1.55858584e-02\n",
      "        -4.37411666e-03 -6.16330002e-03]\n",
      "       [ 1.49184195e-02 -2.46591051e-03  2.39502243e-03 ... -9.06782877e-03\n",
      "        -5.76409558e-03 -5.75604709e-03]]]\n",
      "    \n",
      "    \n",
      "     [[[-1.47162629e-02  4.25466243e-03  6.55410206e-03 ... -7.08849076e-03\n",
      "        -3.86584061e-03  2.83803185e-03]\n",
      "       [-2.22392264e-03 -4.82266676e-03  1.80673767e-02 ... -9.14536510e-03\n",
      "        -1.27943894e-02 -3.44743545e-04]\n",
      "       [-6.33170689e-03 -6.45226147e-03 -3.63600580e-03 ... -4.56040399e-03\n",
      "         7.57330563e-05  6.70851022e-03]]]], bias_init=zeros, format=NCHW>\n",
      "    (1): ReLU<>\n",
      "    (2): MaxPool2d<kernel_size=(49, 1), stride=1, pad_mode=VALID>\n",
      "    >\n",
      "  (layer2): SequentialCell<\n",
      "    (0): Conv2d<input_channels=1, output_channels=96, kernel_size=(4, 40), stride=(1, 1), pad_mode=pad, padding=1, dilation=(1, 1), group=1, has_bias=True, weight_init=[[[[-0.00462703  0.01152247 -0.00497056 ... -0.01737187  0.01389665\n",
      "         0.01078689]\n",
      "       [ 0.00222065  0.00706918 -0.001487   ... -0.01206404 -0.00875248\n",
      "         0.00335742]\n",
      "       [-0.00660563 -0.00835237  0.00538378 ... -0.00186827 -0.01224593\n",
      "         0.01205416]\n",
      "       [ 0.00933064 -0.00551226  0.00350741 ... -0.01106853  0.00058054\n",
      "         0.00684709]]]\n",
      "    \n",
      "    \n",
      "     [[[-0.01649454 -0.00059453 -0.01382058 ... -0.0045323  -0.00052304\n",
      "         0.01051985]\n",
      "       [ 0.0019172   0.00299963  0.01141929 ... -0.01038471  0.00497791\n",
      "         0.00377246]\n",
      "       [ 0.02080176 -0.00080274  0.01677375 ... -0.0061317   0.00513196\n",
      "         0.00088277]\n",
      "       [-0.01134138  0.00434817  0.00894956 ...  0.01292301 -0.02075714\n",
      "        -0.00829834]]]\n",
      "    \n",
      "    \n",
      "     [[[ 0.00965458 -0.01008438 -0.01381588 ...  0.00871198 -0.0139377\n",
      "         0.00456124]\n",
      "       [ 0.00208035 -0.00274053 -0.00034794 ... -0.02208987  0.01050858\n",
      "         0.00906291]\n",
      "       [ 0.00652143 -0.01556316  0.01111745 ...  0.01564477  0.01505133\n",
      "         0.00121292]\n",
      "       [-0.00442901 -0.00163594  0.01298053 ...  0.00621621 -0.00226753\n",
      "        -0.02324785]]]\n",
      "    \n",
      "    \n",
      "     ...\n",
      "    \n",
      "    \n",
      "     [[[-0.00980859 -0.00758316  0.00048921 ...  0.0047823  -0.00421883\n",
      "        -0.00772608]\n",
      "       [ 0.00478056 -0.0060831   0.00189282 ...  0.01048805  0.00460114\n",
      "         0.00675087]\n",
      "       [-0.00564533 -0.01027467 -0.00039618 ...  0.00304475 -0.01190314\n",
      "        -0.0036763 ]\n",
      "       [ 0.01761014 -0.00196972 -0.00358767 ...  0.00470048  0.00978897\n",
      "         0.00455545]]]\n",
      "    \n",
      "    \n",
      "     [[[ 0.00387823  0.00695961  0.00359426 ...  0.00763401  0.01790537\n",
      "        -0.00601015]\n",
      "       [ 0.00293176  0.01290704  0.00995223 ... -0.00719538  0.00080975\n",
      "        -0.01017222]\n",
      "       [ 0.00684088  0.00184011 -0.01271438 ...  0.02121953  0.00084227\n",
      "         0.0203037 ]\n",
      "       [ 0.00013892 -0.0016283   0.00447214 ...  0.00554095  0.00185628\n",
      "        -0.00800288]]]\n",
      "    \n",
      "    \n",
      "     [[[ 0.00932308 -0.01147479 -0.01018782 ...  0.00292371  0.00850409\n",
      "        -0.00697085]\n",
      "       [-0.00149215 -0.00014459  0.01254505 ... -0.00093207 -0.00489767\n",
      "        -0.00805706]\n",
      "       [ 0.01756389 -0.00404738 -0.00077144 ... -0.00423577 -0.00556175\n",
      "        -0.01234256]\n",
      "       [-0.00617097  0.0147011  -0.00200013 ... -0.00272129  0.00539571\n",
      "         0.00584649]]]], bias_init=zeros, format=NCHW>\n",
      "    (1): ReLU<>\n",
      "    (2): MaxPool2d<kernel_size=(48, 1), stride=1, pad_mode=VALID>\n",
      "    >\n",
      "  (layer3): SequentialCell<\n",
      "    (0): Conv2d<input_channels=1, output_channels=96, kernel_size=(5, 40), stride=(1, 1), pad_mode=pad, padding=1, dilation=(1, 1), group=1, has_bias=True, weight_init=[[[[ 4.4553480e-03 -6.2074084e-03  4.4185217e-03 ...  1.2762038e-02\n",
      "        -2.7301442e-03 -1.9975742e-03]\n",
      "       [-1.2326455e-02 -5.6720311e-03 -5.0213127e-03 ... -4.2246355e-04\n",
      "         9.7461250e-03  6.5578078e-03]\n",
      "       [ 3.9168485e-03  6.6873548e-03  6.4897526e-04 ...  1.3151374e-02\n",
      "        -1.2015685e-03  1.1105453e-02]\n",
      "       [-9.3375361e-03 -1.7415725e-02  1.5292932e-02 ... -1.9695887e-02\n",
      "         1.8597152e-02  4.8586326e-03]\n",
      "       [-4.9181092e-03 -5.9259282e-03 -6.4921938e-03 ...  2.3197157e-02\n",
      "         1.3408802e-02 -1.1077950e-02]]]\n",
      "    \n",
      "    \n",
      "     [[[-1.4929696e-02 -4.0756548e-03 -1.2711160e-02 ... -9.7246934e-04\n",
      "         8.8475831e-03  2.0136444e-02]\n",
      "       [ 2.6845955e-03 -9.2299879e-03 -1.0789512e-02 ... -8.5023865e-03\n",
      "        -1.9662792e-02  1.3981629e-02]\n",
      "       [-8.0423476e-03  2.5079283e-03  4.6126958e-04 ...  1.3687081e-02\n",
      "        -9.0709953e-03  7.2697601e-03]\n",
      "       [-3.2650854e-03  9.7514113e-04  1.6447053e-03 ... -1.4381320e-03\n",
      "         4.6323538e-03  8.1646495e-04]\n",
      "       [-1.8715116e-03  1.3993591e-02  3.9695888e-03 ...  1.5774129e-03\n",
      "         1.4088699e-03  1.8161453e-02]]]\n",
      "    \n",
      "    \n",
      "     [[[ 1.3838370e-02 -4.2858785e-03  3.9361967e-03 ... -2.1505614e-03\n",
      "        -1.2012670e-02 -1.0747688e-02]\n",
      "       [-4.6166955e-03  7.1550766e-03 -3.4433319e-03 ... -7.7743856e-03\n",
      "        -1.5942976e-02  1.1090377e-02]\n",
      "       [ 7.0104967e-03  8.6503532e-03 -5.3711270e-04 ...  1.4725771e-02\n",
      "         8.9632804e-03  7.4050878e-04]\n",
      "       [-1.7431818e-02  5.2140574e-03 -1.9424239e-02 ...  4.8636836e-03\n",
      "         8.0166496e-03 -1.4322349e-02]\n",
      "       [-1.2177336e-02  2.8279992e-03  1.3588604e-02 ... -1.0893155e-03\n",
      "        -3.3417519e-03  5.1300912e-03]]]\n",
      "    \n",
      "    \n",
      "     ...\n",
      "    \n",
      "    \n",
      "     [[[ 1.2246403e-03  2.6683053e-03 -3.0246072e-03 ...  1.5383986e-02\n",
      "        -1.1290656e-02 -2.8680416e-03]\n",
      "       [ 1.4160772e-02 -4.9070343e-03 -7.4206847e-03 ... -9.7236289e-03\n",
      "         3.9202720e-03 -2.2198100e-02]\n",
      "       [ 1.1160808e-02  1.1667394e-02 -1.1725996e-03 ...  1.7586138e-02\n",
      "         6.2251496e-03 -1.0981736e-02]\n",
      "       [ 8.4048081e-03  5.9760412e-05  6.8644350e-03 ...  8.9833131e-03\n",
      "         4.5283963e-03 -1.9442396e-02]\n",
      "       [-4.1001891e-05 -1.1579658e-02  1.0924739e-02 ... -1.2643262e-02\n",
      "        -9.9153793e-04 -7.7777551e-03]]]\n",
      "    \n",
      "    \n",
      "     [[[-1.0713284e-02  1.7038744e-02 -6.8603427e-04 ... -8.6446811e-04\n",
      "        -2.7205195e-02 -3.3673935e-03]\n",
      "       [-1.4006880e-02 -5.4066977e-03 -2.0819008e-03 ... -1.7955717e-02\n",
      "        -1.1637325e-02  1.1106579e-02]\n",
      "       [-1.7269833e-02  7.7153533e-03 -4.6531498e-03 ...  6.2427567e-03\n",
      "        -1.6413633e-02 -2.1709291e-02]\n",
      "       [-1.2976493e-02 -3.6917597e-03 -6.2798877e-04 ...  1.7584592e-02\n",
      "        -4.2074365e-03 -2.8578597e-03]\n",
      "       [-2.7579002e-04 -2.4207644e-02 -1.5581613e-03 ... -1.0071398e-02\n",
      "         9.6357995e-05  1.8864481e-03]]]\n",
      "    \n",
      "    \n",
      "     [[[-6.1057261e-03 -1.0808973e-02  3.0745089e-03 ...  1.3340243e-02\n",
      "         1.6339211e-02 -1.6234333e-02]\n",
      "       [ 6.7228191e-03  3.5017221e-03 -1.9025654e-02 ...  1.4490172e-02\n",
      "        -5.6080631e-04  4.6824315e-03]\n",
      "       [-1.1775024e-03  2.0452332e-02  1.1873194e-02 ...  1.1786567e-03\n",
      "         5.2033537e-03  8.6934222e-03]\n",
      "       [ 4.1313656e-03  1.5243866e-03 -9.1060903e-03 ... -6.8475623e-05\n",
      "        -6.2593711e-03  4.3886914e-03]\n",
      "       [-8.3768293e-03  1.5613119e-02 -1.4090891e-03 ... -8.1279678e-03\n",
      "         2.2270663e-04 -5.3487313e-03]]]], bias_init=zeros, format=NCHW>\n",
      "    (1): ReLU<>\n",
      "    (2): MaxPool2d<kernel_size=(47, 1), stride=1, pad_mode=VALID>\n",
      "    >\n",
      "  (fc): Dense<input_channels=288, output_channels=2, has_bias=True>\n",
      "  (drop): Dropout<keep_prob=0.5>\n",
      "  >\n"
     ]
    }
   ],
   "source": [
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Continue training if set pre_trained to be True\n",
    "if cfg.pre_trained:\n",
    "    param_dict = load_checkpoint(cfg.checkpoint_path)\n",
    "    load_param_into_net(net, param_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = nn.Adam(filter(lambda x: x.requires_grad, net.get_parameters()), \n",
    "              learning_rate=learning_rate, weight_decay=cfg.weight_decay)\n",
    "loss = nn.SoftmaxCrossEntropyWithLogits(sparse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(net, loss_fn=loss, optimizer=opt, metrics={'acc': Accuracy()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_ck = CheckpointConfig(save_checkpoint_steps=int(cfg.epoch_size*batch_num/2), keep_checkpoint_max=cfg.keep_checkpoint_max)\n",
    "time_cb = TimeMonitor(data_size=batch_num)\n",
    "ckpt_save_dir = \"./ckpt\"\n",
    "ckpoint_cb = ModelCheckpoint(prefix=\"train_textcnn\", directory=ckpt_save_dir, config=config_ck)\n",
    "loss_cb = LossMonitor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 step: 1192, loss is 0.004214024171233177\n",
      "epoch time: 27643.127 ms, per step time: 23.191 ms\n",
      "epoch: 2 step: 1192, loss is 9.965896606445312e-05\n",
      "epoch time: 9573.943 ms, per step time: 8.032 ms\n",
      "epoch: 3 step: 1192, loss is 0.00023158887051977217\n",
      "epoch time: 9577.438 ms, per step time: 8.035 ms\n",
      "epoch: 4 step: 1192, loss is 9.340495307696983e-05\n",
      "epoch time: 9659.268 ms, per step time: 8.103 ms\n",
      "epoch: 5 step: 1192, loss is 1.4976894817664288e-05\n",
      "epoch time: 9570.284 ms, per step time: 8.029 ms\n",
      "epoch: 6 step: 1192, loss is 1.595295179868117e-05\n",
      "epoch time: 9571.216 ms, per step time: 8.030 ms\n",
      "epoch: 7 step: 1192, loss is 0.0001532187161501497\n",
      "epoch time: 9587.621 ms, per step time: 8.043 ms\n",
      "epoch: 8 step: 1192, loss is 1.0344373549742159e-05\n",
      "epoch time: 9659.592 ms, per step time: 8.104 ms\n",
      "train success\n"
     ]
    }
   ],
   "source": [
    "model.train(cfg.epoch_size, dataset, callbacks=[time_cb, ckpoint_cb, loss_cb])\n",
    "print(\"train success\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 测试评估"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = './ckpt/train_textcnn_6-8_1192.ckpt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load checkpoint from [./ckpt/train_textcnn_6-8_1192.ckpt].\n",
      "accuracy:  {'acc': 0.7587890625}\n"
     ]
    }
   ],
   "source": [
    "dataset = instance.create_test_dataset(batch_size=cfg.batch_size)\n",
    "opt = nn.Adam(filter(lambda x: x.requires_grad, net.get_parameters()), \n",
    "              learning_rate=0.001, weight_decay=cfg.weight_decay)\n",
    "loss = nn.SoftmaxCrossEntropyWithLogits(sparse=True)\n",
    "net = TextCNN(vocab_len=instance.get_dict_len(),word_len=cfg.word_len,\n",
    "                  num_classes=cfg.num_classes,vec_length=cfg.vec_length)\n",
    "\n",
    "if checkpoint_path is not None:\n",
    "    param_dict = load_checkpoint(checkpoint_path)\n",
    "    print(\"load checkpoint from [{}].\".format(checkpoint_path))\n",
    "else:\n",
    "    param_dict = load_checkpoint(cfg.checkpoint_path)\n",
    "    print(\"load checkpoint from [{}].\".format(cfg.checkpoint_path))\n",
    "\n",
    "load_param_into_net(net, param_dict)\n",
    "net.set_train(False)\n",
    "model = Model(net, loss_fn=loss, metrics={'acc': Accuracy()})\n",
    "\n",
    "acc = model.eval(dataset)\n",
    "print(\"accuracy: \", acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. 在线测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(sentence):\n",
    "    sentence = sentence.lower().strip()\n",
    "    sentence = sentence.replace('\\n','')\\\n",
    "                                    .replace('\"','')\\\n",
    "                                    .replace('\\'','')\\\n",
    "                                    .replace('.','')\\\n",
    "                                    .replace(',','')\\\n",
    "                                    .replace('[','')\\\n",
    "                                    .replace(']','')\\\n",
    "                                    .replace('(','')\\\n",
    "                                    .replace(')','')\\\n",
    "                                    .replace(':','')\\\n",
    "                                    .replace('--','')\\\n",
    "                                    .replace('-',' ')\\\n",
    "                                    .replace('\\\\','')\\\n",
    "                                    .replace('0','')\\\n",
    "                                    .replace('1','')\\\n",
    "                                    .replace('2','')\\\n",
    "                                    .replace('3','')\\\n",
    "                                    .replace('4','')\\\n",
    "                                    .replace('5','')\\\n",
    "                                    .replace('6','')\\\n",
    "                                    .replace('7','')\\\n",
    "                                    .replace('8','')\\\n",
    "                                    .replace('9','')\\\n",
    "                                    .replace('`','')\\\n",
    "                                    .replace('=','')\\\n",
    "                                    .replace('$','')\\\n",
    "                                    .replace('/','')\\\n",
    "                                    .replace('*','')\\\n",
    "                                    .replace(';','')\\\n",
    "                                    .replace('<b>','')\\\n",
    "                                    .replace('%','')\\\n",
    "                                    .replace(\"  \",\" \")\n",
    "    sentence = sentence.split(' ')\n",
    "    maxlen = cfg.word_len\n",
    "    vector = [0]*maxlen\n",
    "    for index, word in enumerate(sentence):\n",
    "        if index >= maxlen:\n",
    "            break\n",
    "        if word not in instance.Vocab.keys():\n",
    "            print(word,\"单词未出现在字典中\")\n",
    "        else:\n",
    "            vector[index] = instance.Vocab[word]\n",
    "    sentence = vector\n",
    "\n",
    "    return sentence\n",
    "\n",
    "def inference(review_en):\n",
    "    review_en = preprocess(review_en)\n",
    "    input_en = Tensor(np.array([review_en]).astype(np.int32))\n",
    "    output = net(input_en)\n",
    "    if np.argmax(np.array(output[0])) == 1:\n",
    "        print(\"Positive comments\")\n",
    "    else:\n",
    "        print(\"Negative comments\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.The plot was interesting, but the execution fell short.\n",
      "Negative comments\n",
      "1.The acting was unconventional, which may divide opinions.\n",
      "opinions 单词未出现在字典中\n",
      "Negative comments\n",
      "2.The film had its moments, but overall it left me wanting more.\n",
      "Negative comments\n",
      "3.The characters were complex, yet sometimes felt underdeveloped.\n",
      "Positive comments\n",
      "4.The cinematography was unique, though it may not appeal to everyone.\n",
      "Positive comments\n",
      "5.The story was thought-provoking, but the pacing felt uneven.\n",
      "Negative comments\n",
      "6.The dialogue was sharp, yet occasionally felt forced.\n",
      "Negative comments\n",
      "7.The movie had its ups and downs, making it difficult to form a definitive opinion.\n",
      "Positive comments\n",
      "8.The soundtrack added an interesting layer to the film, but it occasionally felt out of place.\n",
      "Negative comments\n",
      "9.The ending was unexpected, which could be seen as either a strength or a weakness.\n",
      "Positive comments\n"
     ]
    }
   ],
   "source": [
    "comments = [\n",
    "    \"The plot was interesting, but the execution fell short.\",\n",
    "    \"The acting was unconventional, which may divide opinions.\",\n",
    "    \"The film had its moments, but overall it left me wanting more.\",\n",
    "    \"The characters were complex, yet sometimes felt underdeveloped.\",\n",
    "    \"The cinematography was unique, though it may not appeal to everyone.\",\n",
    "    \"The story was thought-provoking, but the pacing felt uneven.\",\n",
    "    \"The dialogue was sharp, yet occasionally felt forced.\",\n",
    "    \"The movie had its ups and downs, making it difficult to form a definitive opinion.\",\n",
    "    \"The soundtrack added an interesting layer to the film, but it occasionally felt out of place.\",\n",
    "    \"The ending was unexpected, which could be seen as either a strength or a weakness.\"\n",
    "]\n",
    "id=0\n",
    "for comment in comments:\n",
    "    print(f\"{id}.{comment}\")\n",
    "    id=id+1\n",
    "    result = inference(comment)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MindSpore",
   "language": "python",
   "name": "mindspore"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "toc-showcode": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
